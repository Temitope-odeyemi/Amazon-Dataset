{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "117529cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "598c5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Amazon_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "41db705d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>price</th>\n",
       "      <th>price (including used books)</th>\n",
       "      <th>pages</th>\n",
       "      <th>avg_reviews</th>\n",
       "      <th>n_reviews</th>\n",
       "      <th>star5</th>\n",
       "      <th>star4</th>\n",
       "      <th>star3</th>\n",
       "      <th>star2</th>\n",
       "      <th>star1</th>\n",
       "      <th>dimensions</th>\n",
       "      <th>weight</th>\n",
       "      <th>language</th>\n",
       "      <th>publisher</th>\n",
       "      <th>ISBN_13</th>\n",
       "      <th>link</th>\n",
       "      <th>complete_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analysis Using R (Low Priced Edition): A ...</td>\n",
       "      <td>[ Dr Dhaval Maheta]</td>\n",
       "      <td>6.75</td>\n",
       "      <td>6.75</td>\n",
       "      <td>500</td>\n",
       "      <td>4.4</td>\n",
       "      <td>23</td>\n",
       "      <td>55%</td>\n",
       "      <td>39%</td>\n",
       "      <td>6%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.5 x 1.01 x 11 inches</td>\n",
       "      <td>2.53 pounds</td>\n",
       "      <td>English</td>\n",
       "      <td>Notion Press Media Pvt Ltd (November 22, 2021)</td>\n",
       "      <td>978-1685549596</td>\n",
       "      <td>/Data-Analysis-Using-Low-Priced/dp/1685549594/...</td>\n",
       "      <td>https://www.amazon.com/Data-Analysis-Using-Low...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Head First Data Analysis: A learner's guide to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.72</td>\n",
       "      <td>21.49 - 33.72</td>\n",
       "      <td>484</td>\n",
       "      <td>4.3</td>\n",
       "      <td>124</td>\n",
       "      <td>61%</td>\n",
       "      <td>20%</td>\n",
       "      <td>9%</td>\n",
       "      <td>4%</td>\n",
       "      <td>6%</td>\n",
       "      <td>8 x 0.98 x 9.25 inches</td>\n",
       "      <td>1.96 pounds</td>\n",
       "      <td>English</td>\n",
       "      <td>O'Reilly Media; 1st edition (August 18, 2009)</td>\n",
       "      <td>978-0596153939</td>\n",
       "      <td>/Head-First-Data-Analysis-statistics/dp/059615...</td>\n",
       "      <td>https://www.amazon.com/Head-First-Data-Analysi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Guerrilla Data Analysis Using Microsoft Excel:...</td>\n",
       "      <td>[ Oz du Soleil,  and , Bill Jelen]</td>\n",
       "      <td>32.07</td>\n",
       "      <td>32.07</td>\n",
       "      <td>274</td>\n",
       "      <td>4.7</td>\n",
       "      <td>10</td>\n",
       "      <td>87%</td>\n",
       "      <td>13%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.25 x 0.6 x 10.75 inches</td>\n",
       "      <td>1.4 pounds</td>\n",
       "      <td>English</td>\n",
       "      <td>Holy Macro! Books; Third edition (August 1, 2022)</td>\n",
       "      <td>978-1615470747</td>\n",
       "      <td>/Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...</td>\n",
       "      <td>https://www.amazon.com/Guerrilla-Analysis-Usin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Data Analysis Using R (Low Priced Edition): A ...   \n",
       "1  Head First Data Analysis: A learner's guide to...   \n",
       "2  Guerrilla Data Analysis Using Microsoft Excel:...   \n",
       "\n",
       "                               author  price price (including used books)  \\\n",
       "0                 [ Dr Dhaval Maheta]   6.75                         6.75   \n",
       "1                                 NaN  33.72               21.49 - 33.72    \n",
       "2  [ Oz du Soleil,  and , Bill Jelen]  32.07                        32.07   \n",
       "\n",
       "  pages  avg_reviews n_reviews star5 star4 star3 star2 star1  \\\n",
       "0   500          4.4        23   55%   39%    6%   NaN   NaN   \n",
       "1   484          4.3       124   61%   20%    9%    4%    6%   \n",
       "2   274          4.7        10   87%   13%   NaN   NaN   NaN   \n",
       "\n",
       "                  dimensions       weight language  \\\n",
       "0     8.5 x 1.01 x 11 inches  2.53 pounds  English   \n",
       "1     8 x 0.98 x 9.25 inches  1.96 pounds  English   \n",
       "2  8.25 x 0.6 x 10.75 inches   1.4 pounds  English   \n",
       "\n",
       "                                           publisher         ISBN_13  \\\n",
       "0     Notion Press Media Pvt Ltd (November 22, 2021)  978-1685549596   \n",
       "1      O'Reilly Media; 1st edition (August 18, 2009)  978-0596153939   \n",
       "2  Holy Macro! Books; Third edition (August 1, 2022)  978-1615470747   \n",
       "\n",
       "                                                link  \\\n",
       "0  /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
       "1  /Head-First-Data-Analysis-statistics/dp/059615...   \n",
       "2  /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
       "\n",
       "                                       complete_link  \n",
       "0  https://www.amazon.com/Data-Analysis-Using-Low...  \n",
       "1  https://www.amazon.com/Head-First-Data-Analysi...  \n",
       "2  https://www.amazon.com/Guerrilla-Analysis-Usin...  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "068a4fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(830, 19)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c2e97ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'author', 'price', 'price (including used books)', 'pages',\n",
       "       'avg_reviews', 'n_reviews', 'star5', 'star4', 'star3', 'star2', 'star1',\n",
       "       'dimensions', 'weight', 'language', 'publisher', 'ISBN_13', 'link',\n",
       "       'complete_link'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "66d45bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 830 entries, 0 to 829\n",
      "Data columns (total 19 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   title                         830 non-null    object \n",
      " 1   author                        657 non-null    object \n",
      " 2   price                         722 non-null    float64\n",
      " 3   price (including used books)  722 non-null    object \n",
      " 4   pages                         745 non-null    object \n",
      " 5   avg_reviews                   702 non-null    float64\n",
      " 6   n_reviews                     702 non-null    object \n",
      " 7   star5                         702 non-null    object \n",
      " 8   star4                         635 non-null    object \n",
      " 9   star3                         554 non-null    object \n",
      " 10  star2                         451 non-null    object \n",
      " 11  star1                         328 non-null    object \n",
      " 12  dimensions                    644 non-null    object \n",
      " 13  weight                        651 non-null    object \n",
      " 14  language                      759 non-null    object \n",
      " 15  publisher                     714 non-null    object \n",
      " 16  ISBN_13                       665 non-null    object \n",
      " 17  link                          830 non-null    object \n",
      " 18  complete_link                 830 non-null    object \n",
      "dtypes: float64(2), object(17)\n",
      "memory usage: 123.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784576db",
   "metadata": {},
   "source": [
    "#### Dropping columns that are  not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7144696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['link', 'complete_link', 'ISBN_13'], axis=1, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4954970d",
   "metadata": {},
   "source": [
    "#### Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a48ba677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>price</th>\n",
       "      <th>price (including used books)</th>\n",
       "      <th>pages</th>\n",
       "      <th>avg_reviews</th>\n",
       "      <th>n_reviews</th>\n",
       "      <th>star5</th>\n",
       "      <th>star4</th>\n",
       "      <th>star3</th>\n",
       "      <th>star2</th>\n",
       "      <th>star1</th>\n",
       "      <th>dimensions</th>\n",
       "      <th>weight</th>\n",
       "      <th>language</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, author, price, price (including used books), pages, avg_reviews, n_reviews, star5, star4, star3, star2, star1, dimensions, weight, language, publisher]\n",
       "Index: []"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()] #checking for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac9337",
   "metadata": {},
   "source": [
    "Since there are no duplicates, we can move on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802673e",
   "metadata": {},
   "source": [
    "#### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "89ebec5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                             0\n",
       "author                          173\n",
       "price                           108\n",
       "price (including used books)    108\n",
       "pages                            85\n",
       "avg_reviews                     128\n",
       "n_reviews                       128\n",
       "star5                           128\n",
       "star4                           195\n",
       "star3                           276\n",
       "star2                           379\n",
       "star1                           502\n",
       "dimensions                      186\n",
       "weight                          179\n",
       "language                         71\n",
       "publisher                       116\n",
       "dtype: int64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517fbac",
   "metadata": {},
   "source": [
    "#### Filling nan cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "07b1ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the empty cells in the author column \n",
    "df['author'].fillna('Not Provided', inplace=True)\n",
    "df['publisher'].fillna('Not Provided', inplace=True)\n",
    "df['language'].fillna('Not_Stated', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7da4b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].fillna('0', inplace=True)\n",
    "df['dimensions'].fillna('0', inplace=True)\n",
    "df['avg_reviews'].fillna('0', inplace=True)\n",
    "df['n_reviews'].fillna('0', inplace=True)\n",
    "df['star5'].fillna('0', inplace=True)\n",
    "df['star4'].fillna('0', inplace=True)\n",
    "df['star3'].fillna('0', inplace=True)\n",
    "df['star2'].fillna('0', inplace=True)\n",
    "df['star1'].fillna('0', inplace=True)\n",
    "df['price (including used books)'].fillna('0', inplace=True)\n",
    "df['pages'].fillna('0', inplace=True)\n",
    "df['weight'].fillna('0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c90a099f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2.53 pounds', '1.96 pounds', '1.4 pounds', '1.47 pounds',\n",
       "       '1.3 pounds', '0', '1.2 pounds', '2.15 pounds', '1.28 pounds',\n",
       "       '2.62 pounds', '1.01 pounds', '7.5 ounces', '1.93 pounds',\n",
       "       '1.75 pounds', '15.5 ounces', '15.2 ounces', '11.4 ounces',\n",
       "       '1.67 pounds', '14.3 ounces', '1.19 pounds', '2.04 pounds',\n",
       "       '1.16 pounds', '3.53 ounces', '9.9 ounces', '1.5 pounds',\n",
       "       '3.04 ounces', '1.54 pounds', '1.14 pounds', '13 ounces',\n",
       "       '1.94 pounds', '2.8 pounds', '1.33 pounds', '14.4 ounces',\n",
       "       '2.36 pounds', '2.05 pounds', '1.45 pounds', '2.4 pounds',\n",
       "       '1.7 pounds', '1.52 pounds', '3.95 pounds', '7.1 ounces',\n",
       "       '1.65 pounds', '1.46 pounds', '1.06 pounds', '2.83 pounds',\n",
       "       '1.41 pounds', '30.2 pounds', '1.85 pounds', '1.51 pounds',\n",
       "       '1 pounds', '1.35 pounds', '4.6 ounces', '2.01 pounds',\n",
       "       '1.66 pounds', '1.1 pounds', '9 ounces', '14.1 ounces',\n",
       "       '2.42 pounds', '1.09 pounds', '1.63 pounds', '1.64 pounds',\n",
       "       '1.6 pounds', '11.5 ounces', '2.48 pounds', '13.9 ounces',\n",
       "       '1.34 pounds', '1.02 pounds', '1.04 pounds', '1.39 pounds',\n",
       "       '1.55 pounds', '1.15 pounds', '4.45 pounds', '1.74 pounds',\n",
       "       '9.6 ounces', '1.98 pounds', '8 ounces', '1.25 pounds',\n",
       "       '1.37 pounds', '4.1 pounds', '11.2 ounces', '1.32 pounds',\n",
       "       '3.46 pounds', '12.8 ounces', '2.16 pounds', '1.11 pounds',\n",
       "       '1.79 pounds', '2.54 pounds', '1.87 pounds', '1.69 pounds',\n",
       "       '3.09 pounds', '12 ounces', '2.4 ounces', '1.08 pounds',\n",
       "       '1.91 pounds', '2.19 pounds', '7.8 ounces', '1.22 pounds',\n",
       "       '4.37 pounds', '1.59 pounds', '4.5 ounces', '1.81 pounds',\n",
       "       '1.07 pounds', '2.35 pounds', '13.3 ounces', '10.2 ounces',\n",
       "       '1.88 pounds', '10.5 ounces', '3.9 pounds', '2.07 pounds',\n",
       "       '1.9 pounds', '1.8 pounds', '16 ounces', '3 pounds', '2.24 ounces',\n",
       "       '3.04 pounds', '1.58 pounds', '13.1 ounces', '13.6 ounces',\n",
       "       '13.8 ounces', '3.15 pounds', '14.5 ounces', '2.77 pounds',\n",
       "       '2.12 pounds', '2.3 pounds', '2.95 pounds', '4.95 pounds',\n",
       "       '1.38 pounds', '2.28 pounds', '1.42 pounds', '3.17 pounds',\n",
       "       '3.35 pounds', '1.71 pounds', '8.8 ounces', '1.95 pounds',\n",
       "       '2.08 pounds', '2.38 pounds', '2.09 pounds', '6.7 ounces',\n",
       "       '2.6 pounds', '14.9 ounces', '2.89 pounds', '1.48 pounds',\n",
       "       '2.39 pounds', '1.83 pounds', '1.26 pounds', '11.3 ounces',\n",
       "       '2.47 pounds', '7 ounces', '5.1 pounds', '3.3 pounds',\n",
       "       '2.87 pounds', '6.1 ounces', '2.29 pounds', '9.8 ounces',\n",
       "       '8.3 ounces', '1.43 pounds', '1.05 pounds', '2 pounds',\n",
       "       '2.93 pounds', '1.27 pounds', '1.72 pounds', '1.89 pounds',\n",
       "       '1.12 pounds', '11.02 pounds', '2.27 pounds', '1.23 pounds',\n",
       "       '12.2 ounces', '14.6 ounces', '2.81 pounds', '15.9 ounces',\n",
       "       '1.13 pounds', '10.4 ounces', '8.2 ounces', '1.76 pounds',\n",
       "       '2.34 pounds', '3.43 pounds', '1.36 pounds', '14.7 ounces',\n",
       "       '3.05 pounds', '1.62 pounds', '2.96 pounds', '1.77 pounds',\n",
       "       '1.92 pounds', '5.3 ounces', '3.51 pounds', '4.12 pounds',\n",
       "       '1.44 pounds', '2.56 pounds', '2.11 pounds', '11.7 ounces',\n",
       "       '2.26 pounds', '3.94 pounds', '2.22 pounds', '3.55 pounds',\n",
       "       '2.24 pounds', '2.5 pounds', '3.7 pounds', '2.84 pounds',\n",
       "       '2.55 pounds', '2.21 pounds', '4.3 ounces', '1.17 pounds',\n",
       "       '1.56 pounds', '2.85 pounds', '2.25 pounds', '15.3 ounces',\n",
       "       '15 ounces', '4.61 pounds', '3.06 pounds', '2.73 pounds',\n",
       "       '13.4 ounces', '9.2 ounces', '14.2 ounces', '2.1 pounds',\n",
       "       '12.5 ounces', '15.8 ounces', '2.56 ounces', '8.1 ounces',\n",
       "       '1.78 pounds', '7.2 ounces', '3.07 pounds', '1.86 pounds',\n",
       "       '1.49 pounds', '2.99 pounds', '5.6 ounces', '2.14 pounds',\n",
       "       '1.21 pounds', '2.45 pounds', '2.66 pounds', '2.2 pounds',\n",
       "       '12.3 ounces', '13.7 ounces', '5.1 ounces', '1.44 ounces',\n",
       "       '1.57 pounds', '1.6 ounces', '6.6 ounces', '3.19 pounds',\n",
       "       '2.03 pounds', '1.03 pounds', '18.62 pounds', '2.32 pounds',\n",
       "       '1.82 pounds', '1.61 pounds', '3.73 pounds', '22.2 pounds',\n",
       "       '9.1 ounces', '2.37 pounds', '1.99 pounds', '2.41 pounds',\n",
       "       '3.52 ounces', '15.7 ounces', '1.29 pounds', '1.73 pounds',\n",
       "       '4.73 pounds', '5.51 pounds',\n",
       "       'I love the way Seth Stephens-Davidowitz explains how we can better live our lives by exploiting the small advantages in life. On the basketball court, I made a career out of finding these types of minor advantages, and I’ve found that most successful individuals in life value the accumulation of small advantages. In the end, they add up to significant life benefits. -- ',\n",
       "       '3.44 pounds', '2.61 pounds', '6.9 ounces', '2.78 pounds',\n",
       "       '14.8 ounces', '3.31 pounds', '11.8 ounces', '0.035 ounces',\n",
       "       '2.69 pounds', '3.25 pounds', '12.6 ounces', '8.4 ounces',\n",
       "       '11 ounces', '14.17 pounds', '4.23 pounds', '2.88 pounds',\n",
       "       '10.9 ounces'], dtype=object)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['weight'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3bed94",
   "metadata": {},
   "source": [
    "#### Cleaning Weight column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "da19c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_converter(item):\n",
    "    if item.split()[-1] == \"pounds\":\n",
    "        return float(item.split()[0]) * 0.45359237\n",
    "    elif item.split()[-1] == \"ounces\":\n",
    "        return float(item.split()[0]) * 0.02834952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3a9323d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight'] = df['weight'].apply(weight_converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade0abe",
   "metadata": {},
   "source": [
    "#### Cleaning Star1, Star2, Star3, Star4, Star5 Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9a82cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_percent(x):\n",
    "    x = x.replace('%', '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9e5023ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['star1'] = df['star1'].apply(remove_percent)\n",
    "df['star2'] = df['star2'].apply(remove_percent)\n",
    "df['star3'] = df['star3'].apply(remove_percent)\n",
    "df['star4'] = df['star4'].apply(remove_percent)\n",
    "df['star5'] = df['star5'].apply(remove_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b5935",
   "metadata": {},
   "source": [
    "#### Cleaning Author Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0827026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_author(x):\n",
    "    if x == x:\n",
    "        return x.lstrip('[ ').rstrip(']').replace(',  and ,', ' and')\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "47a4889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author'] = df['author'].apply(clean_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2753c4",
   "metadata": {},
   "source": [
    "#### Cleaning Language Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e24cc744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['English', 'Not_Stated', 'Spanish',\n",
       "       'Unqualified, Japanese (Dolby Digital 2.0 Mono), English (Dolby Digital 5.1), English (Dolby Digital 2.0 Mono)',\n",
       "       'you will discover all you need ',\n",
       "       '• How to make better business decisions using ',\n",
       "       'Concepts are presented in a \"to-the-point\" style to cater to the busy individual. With this book, you can learn Python in just one day and start coding immediately. ',\n",
       "       'standard library',\n",
       "       'This Python programming guide assumes certain level of programming knowledge. It is not a beginner textbook.',\n",
       "       'Scroll to the top of the page and click the ',\n",
       "       'English (Dolby Digital 2.0 Mono)',\n",
       "       'English (DTS-HD Master Audio 5.1), French (DTS-HD 2.0)',\n",
       "       '\"Brilliant.\"'], dtype=object)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a11afbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_language(row):\n",
    "    lang = row.language\n",
    "    if lang == 'Unqualified, Japanese (Dolby Digital 2.0 Mono), English (Dolby Digital 5.1), English (Dolby Digital 2.0 Mono)' :\n",
    "        row.language = 'English, Japanese'\n",
    "    elif lang == 'English (Dolby Digital 2.0 Mono)' :\n",
    "        row.language = 'English'\n",
    "    elif lang == 'English (DTS-HD Master Audio 5.1), French (DTS-HD 2.0)' :\n",
    "        row.language = 'English, French'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bb434ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(clean_language, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2f963fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_english = df.language.str.contains('English')\n",
    "contains_spanish = df.language.str.contains('Spanish')\n",
    "contains_japanese = df.language.str.contains('Japanese')\n",
    "contains_french = df.language.str.contains('French')\n",
    "contains_Not_Stated = df.language.str.contains('Not_Stated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e5676478",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_language_info = ~(contains_english | contains_spanish | contains_japanese | contains_french | contains_Not_Stated )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "59737919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[not_language_info, ['language']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e5bfd1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['English', 'Not_Stated', 'Spanish', 'English, Japanese', nan,\n",
       "       'English, French'], dtype=object)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad874d",
   "metadata": {},
   "source": [
    "#### Cleaning Dimension Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "64c92d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_conv(item):\n",
    "    if item.split()[:-1].__len__() == 5:\n",
    "        x = float(item.split()[:-1][0]) * 2.54 #cm\n",
    "        y = float(item.split()[:-1][2]) * 2.54  #cm\n",
    "\n",
    "        z = float(item.split()[:-1][4]) * 2.54  #cm\n",
    "        return round(x*y*z, 1)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "49a7a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dimensions'] = df['dimensions'].apply(dimension_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a03c9",
   "metadata": {},
   "source": [
    "#### Change data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8f4e1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['star1'] = df['star1'].astype(\"int\")\n",
    "df['star2'] = df['star2'].astype(\"int\")\n",
    "df['star3'] = df['star3'].astype(\"int\")\n",
    "df['star4'] = df['star4'].astype(\"int\")\n",
    "df['star5'] = df['star5'].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed6074",
   "metadata": {},
   "source": [
    "#### Descibe dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4aa8992d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star5</th>\n",
       "      <th>star4</th>\n",
       "      <th>star3</th>\n",
       "      <th>star2</th>\n",
       "      <th>star1</th>\n",
       "      <th>dimensions</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>830.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>634.000000</td>\n",
       "      <td>650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>61.702410</td>\n",
       "      <td>13.096386</td>\n",
       "      <td>5.778313</td>\n",
       "      <td>2.331325</td>\n",
       "      <td>1.668675</td>\n",
       "      <td>1013.610883</td>\n",
       "      <td>0.761355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>29.635858</td>\n",
       "      <td>10.558259</td>\n",
       "      <td>6.184655</td>\n",
       "      <td>3.266211</td>\n",
       "      <td>2.985264</td>\n",
       "      <td>708.709023</td>\n",
       "      <td>0.847278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.250000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>620.875000</td>\n",
       "      <td>0.423117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>936.600000</td>\n",
       "      <td>0.635029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1301.350000</td>\n",
       "      <td>0.901515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>13273.500000</td>\n",
       "      <td>13.698490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            star5       star4       star3       star2       star1  \\\n",
       "count  830.000000  830.000000  830.000000  830.000000  830.000000   \n",
       "mean    61.702410   13.096386    5.778313    2.331325    1.668675   \n",
       "std     29.635858   10.558259    6.184655    3.266211    2.985264   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%     55.250000    4.250000    0.000000    0.000000    0.000000   \n",
       "50%     70.000000   13.000000    5.000000    1.000000    0.000000   \n",
       "75%     79.000000   19.000000    9.000000    3.000000    3.000000   \n",
       "max    100.000000   64.000000   49.000000   22.000000   29.000000   \n",
       "\n",
       "         dimensions      weight  \n",
       "count    634.000000  650.000000  \n",
       "mean    1013.610883    0.761355  \n",
       "std      708.709023    0.847278  \n",
       "min        0.000000    0.000992  \n",
       "25%      620.875000    0.423117  \n",
       "50%      936.600000    0.635029  \n",
       "75%     1301.350000    0.901515  \n",
       "max    13273.500000   13.698490  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56193a7",
   "metadata": {},
   "source": [
    "#### Save cleaned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "04df5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Amazon_data_science_books_cleaned_file.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
